{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.dates as mdates\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import topics data.\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords')\n",
    "folders = os.listdir()\n",
    "for x in folders: exec(x + ' = pd.DataFrame()')\n",
    "    \n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t0')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t0 = t0.append(df_temp)\n",
    "    df_temp = [] \n",
    "t0 = t0.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t1')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t1 = t1.append(df_temp)\n",
    "    df_temp = []\n",
    "t1 = t1.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t2')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t2 = t2.append(df_temp)\n",
    "    df_temp = []\n",
    "t2 = t2.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t3')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t3 = t3.append(df_temp)\n",
    "    df_temp = []\n",
    "t3 = t3.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t4')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t4 = t4.append(df_temp)\n",
    "    df_temp = []\n",
    "t4 = t4.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t5')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t5 = t5.append(df_temp)\n",
    "    df_temp = []\n",
    "t5 = t5.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t6')\n",
    "\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t6 = t6.append(df_temp)\n",
    "    df_temp = []\n",
    "t6 = t6.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t7')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t7 = t7.append(df_temp)\n",
    "    df_temp = []\n",
    "t7 = t7.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t8')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t8 = t8.append(df_temp)\n",
    "    df_temp = []\n",
    "t8 = t8.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t9')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t9 = t9.append(df_temp)\n",
    "    df_temp = []\n",
    "t9 = t9.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "result = t0.append([t1,t2,t3,t4,t5,t6,t7,t8,t9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we drop unnecessary columns.\n",
    "result.dropna(subset = [\"count\"], inplace=True)\n",
    "result = result.drop('relevance', axis=1)\n",
    "result.reset_index(level=0, inplace=True)\n",
    "result = result.drop('index', axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text               0\n",
       "count              0\n",
       "sentiment_score    0\n",
       "sentiment_label    0\n",
       "date               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we check if there is NaN in data frame.\n",
    "result.columns = result.columns.str.replace(r\" \", \"_\")\n",
    "result.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Clean Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Delete unnecessary elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We delete some unnecessary elements.\n",
    "result['text'] = result['text'].str.replace('#','')\n",
    "result['text'] = result['text'].str.replace('【','')\n",
    "result['text'] = result['text'].str.replace('】','')\n",
    "result['text'] = result['text'].str.replace('／','')\n",
    "result['text'] = result['text'].str.replace('（','')\n",
    "result['text'] = result['text'].str.replace('）','')\n",
    "result['text'] = result['text'].str.replace('　','')\n",
    "result['text'] = result['text'].str.replace(' ','')\n",
    "result['text'] = result['text'].str.replace('#','')\n",
    "result['text'] = result['text'].str.replace('〜','')\n",
    "result['text'] = result['text'].str.replace('＜','')\n",
    "result['text'] = result['text'].str.replace('＞','')\n",
    "result['text'] = result['text'].str.replace('％','')\n",
    "result['text'] = result['text'].str.replace('＃','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Alter Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we combine some keywords with same meanings but different expressions. \n",
    "\n",
    "def merge_keywords(row):\n",
    "    if 'コロナ' in row['text'] or '肺炎' in row['text'] or '新型ウイルス' in row['text']:val = '新型コロナウイルス'    \n",
    "    elif '中国' in row['text']:val = '中国'      \n",
    "    elif '武漢' in row['text'] or '湖北' in row['text']:\n",
    "        val = '武漢'\n",
    "    elif '香港' in row['text']:\n",
    "        val = '香港'\n",
    "    elif 'マスク' in row['text']:\n",
    "        val = 'マスク'        \n",
    "    elif 'ワクチン' in row['text']:\n",
    "        val = 'ワクチン'  \n",
    "    elif 'プレゼント' in row['text'] or '🧧' in row['text'] or '🎁' in row['text']:\n",
    "        val = 'プレゼント'   \n",
    "    elif 'クリスマス' in row['text'] or 'Xmas' in row['text'] or '🎅' in row['text'] or '🎄' in row['text']:\n",
    "        val = 'クリスマス'\n",
    "        \n",
    "    elif 'キャンペーン' in row['text']:\n",
    "        val = 'キャンペーン'         \n",
    "    elif 'RT' in row['text']:\n",
    "        val = 'RT'                   \n",
    "    elif 'フォロー' in row['text']:\n",
    "        val = 'フォロー' \n",
    "    elif 'フォロワー' in row['text']:\n",
    "        val = 'フォロワー' \n",
    "    elif '応募' in row['text']:\n",
    "        val = '応募'        \n",
    "    elif '抽選' in row['text']:\n",
    "        val = '抽選'                \n",
    "    elif '締切' in row['text']:\n",
    "        val = '締切'                \n",
    "    elif '新年' in row['text'] or '元旦' in row['text'] or '新春' in row['text'] or '🎍' in row['text']:\n",
    "        val = '新年'        \n",
    "    elif '年越' in row['text']:\n",
    "        val = '年越し'        \n",
    "    elif '正月' in row['text']:\n",
    "        val = '正月'        \n",
    "    elif '年末' in row['text']:\n",
    "        val = '年末'          \n",
    "    elif 'ゴーン' in row['text']:\n",
    "        val = 'ゴーン被告'   \n",
    "    \n",
    "    elif 'あつ森' in row['text'] or 'ぶつの森' in row['text']:\n",
    "        val = 'どうぶつの森'  \n",
    "        \n",
    "    elif 'おつかれ' in row['text']:\n",
    "        val = 'おつかれ'          \n",
    "    elif 'お願い' in row['text']:\n",
    "        val = 'お願い'          \n",
    "    elif 'ご苦労' in row['text']:\n",
    "        val = 'ご苦労'  \n",
    "        \n",
    "    elif 'ももクロ' in row['text']:\n",
    "        val = 'ももクロ'  \n",
    "    elif '嵐' in row['text'] or 'ARASHI' in row['text']:\n",
    "        val = '嵐'  \n",
    "    elif '相葉' in row['text']:\n",
    "        val = '相葉雅紀'  \n",
    "    elif '翔' in row['text'] or '櫻井' in row['text']:\n",
    "        val = '櫻井翔'          \n",
    "    elif '松本潤' in row['text']:\n",
    "        val = '松本潤'  \n",
    "    elif '二宮和也' in row['text'] or '二宮' in row['text']:\n",
    "        val = '二宮和也'  \n",
    "    elif '大野智' in row['text'] or '大ちゃん' in row['text']:\n",
    "        val = '大野智'  \n",
    "    elif '香取慎吾' in row['text'] or '大ちゃん' in row['text']:\n",
    "        val = '香取慎吾'     \n",
    "\n",
    "    elif '菅官房長官' in row['text'] or '菅氏' in row['text'] or '菅義偉' in row['text']:\n",
    "        val = '菅義偉'\n",
    "    elif '安倍' in row['text']:\n",
    "        val = '安倍'    \n",
    "    elif '紅白' in row['text']:\n",
    "        val = '紅白'              \n",
    "        \n",
    "    elif '鬼滅' in row['text']:\n",
    "        val = '鬼滅'   \n",
    "    elif '炭治郎' in row['text']:\n",
    "        val = '炭治郎'   \n",
    "    \n",
    "    elif 'センター' in row['text'] or 'センター試験' in row['text']:\n",
    "        val = 'センター試験'   \n",
    "        \n",
    "    elif 'ポケットモンスター' in row['text'] or 'ポケモン' in row['text']:\n",
    "        val = 'ポケモン'         \n",
    "    elif '麒麟' in row['text']:\n",
    "        val = '麒麟'  \n",
    "    elif '毎日挑戦' in row['text']:\n",
    "        val = '毎日挑戦'  \n",
    "    elif 'チャレンジ' in row['text']:\n",
    "        val = 'チャレンジ'\n",
    "    elif 'チャンス' in row['text']:\n",
    "        val = 'チャンス'   \n",
    "    elif '再挑戦' in row['text']:\n",
    "        val = '再挑戦'           \n",
    "    \n",
    "    elif '今日' in row['text']:\n",
    "        val = '今日'        \n",
    "    elif 'ぶり' in row['text']:\n",
    "        val = 'ぶり'  \n",
    "    elif 'wPeing' in row['text']:\n",
    "        val = 'Peing'  \n",
    "    elif 'Д' in row['text'] or 'д' in row['text'] :\n",
    "        val = '゜Д゜' \n",
    "    elif '大輔' in row['text'] or '高橋大輔' in row['text']:\n",
    "        val = '高橋大輔'  \n",
    "    elif '菅田将暉' in row['text'] or '菅田くん' in row['text']:\n",
    "        val = '菅田将暉'  \n",
    "    elif '環奈' in row['text']:\n",
    "        val = '橋本環奈'  \n",
    "    elif 'ツイート数' in row['text']:\n",
    "        val = 'ツイート数' \n",
    "    elif '発売' in row['text']:\n",
    "        val = '発売'          \n",
    "    elif 'Mステ' in row['text']:\n",
    "        val = 'Mステ'    \n",
    "    elif 'お誕生日' in row['text']:\n",
    "        val = '誕生日' \n",
    "    elif 'Amazonギフト券' in row['text'] or 'Amazonギフト' in row['text']:\n",
    "        val = 'Amazonギフト券'\n",
    "    elif 'オリンピック' in row['text']:\n",
    "        val = 'オリンピック'\n",
    "    elif 'ω' in row['text'] or '꒳' in row['text']:\n",
    "        val = 'ω'        \n",
    "    elif row['text']=='年1月':\n",
    "        val = '1月'  \n",
    "    elif row['text']=='年2月':\n",
    "        val = '2月' \n",
    "    elif row['text']=='年3月':\n",
    "        val = '3月' \n",
    "    elif row['text']=='年4月':\n",
    "        val = '4月' \n",
    "    elif row['text']=='年12月':\n",
    "        val = '12月'                        \n",
    "    elif row['text']=='日午後':\n",
    "        val = '午後'  \n",
    "    else:\n",
    "        val = row['text']\n",
    "    return val\n",
    "\n",
    "result['text'] = result.apply(merge_keywords, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We won't be using Emojis as part of our analysis, so we delete them here.\n",
    "\n",
    "To_remove = ['👉','💫','👏🏻','👏','😭','🎉','🤣',\n",
    "             '😅','💦','🍻','😍','❤️','🦀','🐘',\n",
    "             '🏆','🌟','😆','👍','🎵','🎷','🎶',\n",
    "             '🥁','🦠','💕','👋','📣','🌸','🎯',\n",
    "             '😁','👍','⚠️','😃','❗️','💙','💿',\n",
    "             '🙇‍','🙇‍♀️','🥟','🥺','🌺','💕','🐴',\n",
    "             '💡','📢','🔥','😊','🤛','😂','🤭','✌️',\n",
    "             '❣️','⭐️','🍚','💐','㊗️','🎊','🔁','👑',\n",
    "             '🍀', '🎂', '💛', '💜', '😡',\n",
    "             '日目','回目','枚目','さん','号店','歳児',\n",
    "             '位','回話','名様','名さま','m','￣Y']\n",
    "\n",
    "for i in To_remove:\n",
    "    result['text'] = result['text'].str.replace(i,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sentiment Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create \"negative_label_count\", \"neutral_label_count\" and \"positive_label_count\" to represent daily count of 3 labels.\n",
    "sentiment_label = pd.crosstab(result.sentiment_label,result.date).T\n",
    "for x in sentiment_label:\n",
    "    sentiment_label = sentiment_label.rename(columns={x: x+'_label_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Negativity in Every Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.get_dummies(result, columns=['sentiment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 22100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we craete binary variable 'sentiment_label_negative' to represent if a keyword displayed negativity on that day. \n",
    "\n",
    "sentiment_label_negative = pd.DataFrame(result.groupby(['date', 'text'])['sentiment_label_negative'].mean())\n",
    "sentiment_label_negative = sentiment_label_negative.unstack(level=0).T\n",
    "sentiment_label_negative.reset_index(level=0, inplace=True)\n",
    "sentiment_label_negative = sentiment_label_negative.drop('level_0',axis=1)\n",
    "sentiment_label_negative = sentiment_label_negative.iloc[:,1:]\n",
    "sentiment_label_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 578)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete columns containing either 95% or more NaN Values\n",
    "perc = 95\n",
    "min_count =  int(((100-perc)/100)*sentiment_label_negative.shape[0] + 1)\n",
    "sentiment_label_negative = sentiment_label_negative.dropna( axis=1, thresh=min_count)\n",
    "sentiment_label_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 578)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_label_negative = sentiment_label_negative.replace(np.nan, 0)\n",
    "sentiment_label_negative[sentiment_label_negative != 0] = 1\n",
    "sentiment_label_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sentiment_label_negative:\n",
    "    sentiment_label_negative = sentiment_label_negative.rename(columns={x: 'negative '+x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Positivity in Every Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we craete binary variable 'sentiment_label_positive' to represent if a keyword displayed positivity on that day. \n",
    "sentiment_label_positive = pd.DataFrame(result.groupby(['date', 'text'])['sentiment_label_positive'].mean())\n",
    "sentiment_label_positive = sentiment_label_positive.unstack(level=0).T\n",
    "sentiment_label_positive.reset_index(level=0, inplace=True)\n",
    "sentiment_label_positive = sentiment_label_positive.drop('level_0',axis=1)\n",
    "sentiment_label_positive = sentiment_label_positive.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 578)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete columns containing either 95% or more NaN Values\n",
    "perc = 95\n",
    "min_count =  int(((100-perc)/100)*sentiment_label_positive.shape[0] + 1)\n",
    "sentiment_label_positive = sentiment_label_positive.dropna( axis=1, thresh=min_count)\n",
    "sentiment_label_positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_label_positive = sentiment_label_positive.replace(np.nan, 0)\n",
    "sentiment_label_positive[sentiment_label_positive != 0] = 1\n",
    "\n",
    "for x in sentiment_label_positive:\n",
    "    sentiment_label_positive = sentiment_label_positive.rename(columns={x: 'positive '+x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Daily Average Overall Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score = result.groupby('date').mean()['sentiment_score'].reset_index()\n",
    "sentiment_score = sentiment_score.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Daily Keyword Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_count = pd.DataFrame(result.groupby(['date', 'text'])['count'].sum())\n",
    "keyword_count = keyword_count.unstack(level=0).T\n",
    "keyword_count.reset_index(level=0, inplace=True)\n",
    "keyword_count = keyword_count.drop('level_0',axis=1)\n",
    "keyword_count = keyword_count.iloc[:, 1:]\n",
    "for x in keyword_count:\n",
    "    keyword_count = keyword_count.rename(columns={x: 'count '+x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns containing either 95% or more NaN Values\n",
    "perc = 95\n",
    "min_count =  int(((100-perc)/100)*keyword_count.shape[0] + 1)\n",
    "keyword_count = keyword_count.dropna( axis=1, thresh=min_count)\n",
    "\n",
    "keyword_count = keyword_count.replace(np.nan, 0)\n",
    "keyword_count[keyword_count != 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Create Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we cobine all variables created before and remve constant features in the dataframe. \n",
    "dfALL = pd.concat([sentiment_label, sentiment_label_negative, sentiment_label_positive, keyword_count], axis=1)\n",
    "dfALL = dfALL.replace([np.inf, -np.inf], np.nan)\n",
    "dfALL = dfALL.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 1528)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfALL = dfALL.loc[:, (dfALL != 0).any(axis=0)]\n",
    "dfALL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Percentage Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 581)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nu = pd.concat([sentiment_label, keyword_count], axis=1)\n",
    "df_nu = df_nu.replace([np.inf, -np.inf], np.nan)\n",
    "df_nu = df_nu.fillna(0)\n",
    "df_nu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 581)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we check if there is any empty column.\n",
    "df_nu = df_nu.loc[:, (df_nu != 0).any(axis=0)]\n",
    "df_nu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 581)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfALL_pct = df_nu.pct_change()\n",
    "\n",
    "for x in dfALL_pct:\n",
    "    dfALL_pct = dfALL_pct.rename(columns={x: 'pct '+x}) \n",
    "\n",
    "dfALL_pct = dfALL_pct.replace([np.inf, -np.inf], np.nan)\n",
    "dfALL_pct = dfALL_pct.fillna(0)\n",
    "dfALL_pct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lags ####\n",
    "LAG1 = dfALL.shift(periods=1) \n",
    "LAG2 = dfALL.shift(periods=2) \n",
    "LAG3 = dfALL.shift(periods=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lag difference ####\n",
    "LAG_nu_1 = df_nu.shift(periods=1) \n",
    "LAG_nu_2 = df_nu.shift(periods=2) \n",
    "LAG_nu_3 = df_nu.shift(periods=3) \n",
    "\n",
    "diff_LAG1 = LAG_nu_1 - df_nu\n",
    "diff_LAG2 = LAG_nu_2 - df_nu\n",
    "diff_LAG3 = LAG_nu_3 - df_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in LAG1:\n",
    "    LAG1 = LAG1.rename(columns={x: 'Lag1 '+x}) \n",
    "for x in LAG2:\n",
    "    LAG2 = LAG2.rename(columns={x: 'Lag2 '+x})\n",
    "for x in LAG3:\n",
    "    LAG3 = LAG3.rename(columns={x: 'Lag3 '+x})\n",
    "    \n",
    "for x in diff_LAG1:\n",
    "    diff_LAG1 = diff_LAG1.rename(columns={x: 'DiffLag1 '+x}) \n",
    "for x in diff_LAG2:\n",
    "    diff_LAG2 = diff_LAG2.rename(columns={x: 'DiffLag2 '+x}) \n",
    "for x in diff_LAG3:\n",
    "    diff_LAG3 = diff_LAG3.rename(columns={x: 'DiffLag3 '+x}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Rolling Windows Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### rolling 3 ####\n",
    "ROLL_3 = df_nu.rolling(3, win_type='triang').mean()\n",
    "#### rolling difference #### \n",
    "DiffRoll3 = ROLL_3 - df_nu\n",
    "\n",
    "for x in ROLL_3:\n",
    "    ROLL_3 = ROLL_3.rename(columns={x: 'Roll3 '+x}) \n",
    "for x in DiffRoll3:\n",
    "    DiffRoll3 = DiffRoll3.rename(columns={x: 'DiffRoll3 '+x}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Output All Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfALL = pd.concat([dfALL, dfALL_pct, LAG1, LAG2, LAG3, diff_LAG1, diff_LAG2, diff_LAG3, ROLL_3, DiffRoll3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 9554)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfALL = dfALL.iloc[3:,:]\n",
    "dfALL = dfALL.replace([np.inf, -np.inf], np.nan)\n",
    "dfALL = dfALL.replace(np.nan, 0)\n",
    "\n",
    "dfALL = dfALL.loc[:, (dfALL != 0).any(axis=0)]\n",
    "dfALL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 9554)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we check if there is any empty column.\n",
    "dfALL = dfALL.loc[:, (dfALL != 0).any(axis=0)]\n",
    "dfALL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 9554)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete columns containing either 95% or more NaN Values\n",
    "perc = 95\n",
    "min_count =  int(((100-perc)/100)*dfALL.shape[0] + 1)\n",
    "dfALL = dfALL.dropna( axis=1, thresh=min_count)\n",
    "dfALL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Label Variables \n",
    "Because R studio cannot display Japanese characters, here we transfer variable names to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\processed data')\n",
    "\n",
    "keyword_list = dfALL.columns.to_list()\n",
    "keyword_list = pd.DataFrame(keyword_list)\n",
    "keyword_list.to_excel('keyword_list_correct.xlsx')\n",
    "\n",
    "df_rename = dfALL\n",
    "for x in range(0,8319):\n",
    "    df_rename.rename(columns={ df_rename.columns[x]: 'keywords_df'+str(x) }, inplace = True) \n",
    "df_rename.to_csv('KEYWORDS_with_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Check Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes long time, so we won't run it in the demonstration. \n",
    "\n",
    "#os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\processed data')\n",
    "\n",
    "#target = pd.read_csv('SALES.csv')\n",
    "\n",
    "#col_choice = dfALL.columns\n",
    "\n",
    "#for x in col_choice:\n",
    "    #plt.scatter(dfALL[x], target['spike'])\n",
    "    #plt.title(x)\n",
    "    #plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
