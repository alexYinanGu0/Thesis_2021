{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.dates as mdates\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import topics data.\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords')\n",
    "folders = os.listdir()\n",
    "for x in folders: exec(x + ' = pd.DataFrame()')\n",
    "    \n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t0')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t0 = t0.append(df_temp)\n",
    "    df_temp = [] \n",
    "t0 = t0.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t1')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t1 = t1.append(df_temp)\n",
    "    df_temp = []\n",
    "t1 = t1.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t2')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t2 = t2.append(df_temp)\n",
    "    df_temp = []\n",
    "t2 = t2.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t3')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t3 = t3.append(df_temp)\n",
    "    df_temp = []\n",
    "t3 = t3.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t4')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t4 = t4.append(df_temp)\n",
    "    df_temp = []\n",
    "t4 = t4.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t5')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t5 = t5.append(df_temp)\n",
    "    df_temp = []\n",
    "t5 = t5.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t6')\n",
    "\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t6 = t6.append(df_temp)\n",
    "    df_temp = []\n",
    "t6 = t6.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t7')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t7 = t7.append(df_temp)\n",
    "    df_temp = []\n",
    "t7 = t7.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t8')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t8 = t8.append(df_temp)\n",
    "    df_temp = []\n",
    "t8 = t8.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\raw data\\\\keywords\\\\t9')\n",
    "files = os.listdir()\n",
    "for f in files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_temp['date'] = f[:10]\n",
    "    t9 = t9.append(df_temp)\n",
    "    df_temp = []\n",
    "t9 = t9.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "result = t0.append([t1,t2,t3,t4,t5,t6,t7,t8,t9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we drop unnecessary columns.\n",
    "result.dropna(subset = [\"count\"], inplace=True)\n",
    "result = result.drop('relevance', axis=1)\n",
    "result.reset_index(level=0, inplace=True)\n",
    "result = result.drop('index', axis=1)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text               0\n",
       "count              0\n",
       "sentiment_score    0\n",
       "sentiment_label    0\n",
       "date               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we check if there is NaN in data frame.\n",
    "result.columns = result.columns.str.replace(r\" \", \"_\")\n",
    "result.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Clean Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Delete unnecessary elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We delete some unnecessary elements.\n",
    "result['text'] = result['text'].str.replace('#','')\n",
    "result['text'] = result['text'].str.replace('ã€','')\n",
    "result['text'] = result['text'].str.replace('ã€‘','')\n",
    "result['text'] = result['text'].str.replace('ï¼','')\n",
    "result['text'] = result['text'].str.replace('ï¼ˆ','')\n",
    "result['text'] = result['text'].str.replace('ï¼‰','')\n",
    "result['text'] = result['text'].str.replace('ã€€','')\n",
    "result['text'] = result['text'].str.replace(' ','')\n",
    "result['text'] = result['text'].str.replace('#','')\n",
    "result['text'] = result['text'].str.replace('ã€œ','')\n",
    "result['text'] = result['text'].str.replace('ï¼œ','')\n",
    "result['text'] = result['text'].str.replace('ï¼','')\n",
    "result['text'] = result['text'].str.replace('ï¼…','')\n",
    "result['text'] = result['text'].str.replace('ï¼ƒ','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Alter Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we combine some keywords with same meanings but different expressions. \n",
    "\n",
    "def merge_keywords(row):\n",
    "    if 'ã‚³ãƒ­ãƒŠ' in row['text'] or 'è‚ºç‚' in row['text'] or 'æ–°å‹ã‚¦ã‚¤ãƒ«ã‚¹' in row['text']:val = 'æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹'    \n",
    "    elif 'ä¸­å›½' in row['text']:val = 'ä¸­å›½'      \n",
    "    elif 'æ­¦æ¼¢' in row['text'] or 'æ¹–åŒ—' in row['text']:\n",
    "        val = 'æ­¦æ¼¢'\n",
    "    elif 'é¦™æ¸¯' in row['text']:\n",
    "        val = 'é¦™æ¸¯'\n",
    "    elif 'ãƒã‚¹ã‚¯' in row['text']:\n",
    "        val = 'ãƒã‚¹ã‚¯'        \n",
    "    elif 'ãƒ¯ã‚¯ãƒãƒ³' in row['text']:\n",
    "        val = 'ãƒ¯ã‚¯ãƒãƒ³'  \n",
    "    elif 'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ' in row['text'] or 'ğŸ§§' in row['text'] or 'ğŸ' in row['text']:\n",
    "        val = 'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ'   \n",
    "    elif 'ã‚¯ãƒªã‚¹ãƒã‚¹' in row['text'] or 'Xmas' in row['text'] or 'ğŸ…' in row['text'] or 'ğŸ„' in row['text']:\n",
    "        val = 'ã‚¯ãƒªã‚¹ãƒã‚¹'\n",
    "        \n",
    "    elif 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³' in row['text']:\n",
    "        val = 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³'         \n",
    "    elif 'RT' in row['text']:\n",
    "        val = 'RT'                   \n",
    "    elif 'ãƒ•ã‚©ãƒ­ãƒ¼' in row['text']:\n",
    "        val = 'ãƒ•ã‚©ãƒ­ãƒ¼' \n",
    "    elif 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼' in row['text']:\n",
    "        val = 'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼' \n",
    "    elif 'å¿œå‹Ÿ' in row['text']:\n",
    "        val = 'å¿œå‹Ÿ'        \n",
    "    elif 'æŠ½é¸' in row['text']:\n",
    "        val = 'æŠ½é¸'                \n",
    "    elif 'ç· åˆ‡' in row['text']:\n",
    "        val = 'ç· åˆ‡'                \n",
    "    elif 'æ–°å¹´' in row['text'] or 'å…ƒæ—¦' in row['text'] or 'æ–°æ˜¥' in row['text'] or 'ğŸ' in row['text']:\n",
    "        val = 'æ–°å¹´'        \n",
    "    elif 'å¹´è¶Š' in row['text']:\n",
    "        val = 'å¹´è¶Šã—'        \n",
    "    elif 'æ­£æœˆ' in row['text']:\n",
    "        val = 'æ­£æœˆ'        \n",
    "    elif 'å¹´æœ«' in row['text']:\n",
    "        val = 'å¹´æœ«'          \n",
    "    elif 'ã‚´ãƒ¼ãƒ³' in row['text']:\n",
    "        val = 'ã‚´ãƒ¼ãƒ³è¢«å‘Š'   \n",
    "    \n",
    "    elif 'ã‚ã¤æ£®' in row['text'] or 'ã¶ã¤ã®æ£®' in row['text']:\n",
    "        val = 'ã©ã†ã¶ã¤ã®æ£®'  \n",
    "        \n",
    "    elif 'ãŠã¤ã‹ã‚Œ' in row['text']:\n",
    "        val = 'ãŠã¤ã‹ã‚Œ'          \n",
    "    elif 'ãŠé¡˜ã„' in row['text']:\n",
    "        val = 'ãŠé¡˜ã„'          \n",
    "    elif 'ã”è‹¦åŠ´' in row['text']:\n",
    "        val = 'ã”è‹¦åŠ´'  \n",
    "        \n",
    "    elif 'ã‚‚ã‚‚ã‚¯ãƒ­' in row['text']:\n",
    "        val = 'ã‚‚ã‚‚ã‚¯ãƒ­'  \n",
    "    elif 'åµ' in row['text'] or 'ARASHI' in row['text']:\n",
    "        val = 'åµ'  \n",
    "    elif 'ç›¸è‘‰' in row['text']:\n",
    "        val = 'ç›¸è‘‰é›…ç´€'  \n",
    "    elif 'ç¿”' in row['text'] or 'æ«»äº•' in row['text']:\n",
    "        val = 'æ«»äº•ç¿”'          \n",
    "    elif 'æ¾æœ¬æ½¤' in row['text']:\n",
    "        val = 'æ¾æœ¬æ½¤'  \n",
    "    elif 'äºŒå®®å’Œä¹Ÿ' in row['text'] or 'äºŒå®®' in row['text']:\n",
    "        val = 'äºŒå®®å’Œä¹Ÿ'  \n",
    "    elif 'å¤§é‡æ™º' in row['text'] or 'å¤§ã¡ã‚ƒã‚“' in row['text']:\n",
    "        val = 'å¤§é‡æ™º'  \n",
    "    elif 'é¦™å–æ…å¾' in row['text'] or 'å¤§ã¡ã‚ƒã‚“' in row['text']:\n",
    "        val = 'é¦™å–æ…å¾'     \n",
    "\n",
    "    elif 'è…å®˜æˆ¿é•·å®˜' in row['text'] or 'è…æ°' in row['text'] or 'è…ç¾©å‰' in row['text']:\n",
    "        val = 'è…ç¾©å‰'\n",
    "    elif 'å®‰å€' in row['text']:\n",
    "        val = 'å®‰å€'    \n",
    "    elif 'ç´…ç™½' in row['text']:\n",
    "        val = 'ç´…ç™½'              \n",
    "        \n",
    "    elif 'é¬¼æ»…' in row['text']:\n",
    "        val = 'é¬¼æ»…'   \n",
    "    elif 'ç‚­æ²»éƒ' in row['text']:\n",
    "        val = 'ç‚­æ²»éƒ'   \n",
    "    \n",
    "    elif 'ã‚»ãƒ³ã‚¿ãƒ¼' in row['text'] or 'ã‚»ãƒ³ã‚¿ãƒ¼è©¦é¨“' in row['text']:\n",
    "        val = 'ã‚»ãƒ³ã‚¿ãƒ¼è©¦é¨“'   \n",
    "        \n",
    "    elif 'ãƒã‚±ãƒƒãƒˆãƒ¢ãƒ³ã‚¹ã‚¿ãƒ¼' in row['text'] or 'ãƒã‚±ãƒ¢ãƒ³' in row['text']:\n",
    "        val = 'ãƒã‚±ãƒ¢ãƒ³'         \n",
    "    elif 'éº’éºŸ' in row['text']:\n",
    "        val = 'éº’éºŸ'  \n",
    "    elif 'æ¯æ—¥æŒ‘æˆ¦' in row['text']:\n",
    "        val = 'æ¯æ—¥æŒ‘æˆ¦'  \n",
    "    elif 'ãƒãƒ£ãƒ¬ãƒ³ã‚¸' in row['text']:\n",
    "        val = 'ãƒãƒ£ãƒ¬ãƒ³ã‚¸'\n",
    "    elif 'ãƒãƒ£ãƒ³ã‚¹' in row['text']:\n",
    "        val = 'ãƒãƒ£ãƒ³ã‚¹'   \n",
    "    elif 'å†æŒ‘æˆ¦' in row['text']:\n",
    "        val = 'å†æŒ‘æˆ¦'           \n",
    "    \n",
    "    elif 'ä»Šæ—¥' in row['text']:\n",
    "        val = 'ä»Šæ—¥'        \n",
    "    elif 'ã¶ã‚Š' in row['text']:\n",
    "        val = 'ã¶ã‚Š'  \n",
    "    elif 'wPeing' in row['text']:\n",
    "        val = 'Peing'  \n",
    "    elif 'Ğ”' in row['text'] or 'Ğ´' in row['text'] :\n",
    "        val = 'ã‚œĞ”ã‚œ' \n",
    "    elif 'å¤§è¼”' in row['text'] or 'é«˜æ©‹å¤§è¼”' in row['text']:\n",
    "        val = 'é«˜æ©‹å¤§è¼”'  \n",
    "    elif 'è…ç”°å°†æš‰' in row['text'] or 'è…ç”°ãã‚“' in row['text']:\n",
    "        val = 'è…ç”°å°†æš‰'  \n",
    "    elif 'ç’°å¥ˆ' in row['text']:\n",
    "        val = 'æ©‹æœ¬ç’°å¥ˆ'  \n",
    "    elif 'ãƒ„ã‚¤ãƒ¼ãƒˆæ•°' in row['text']:\n",
    "        val = 'ãƒ„ã‚¤ãƒ¼ãƒˆæ•°' \n",
    "    elif 'ç™ºå£²' in row['text']:\n",
    "        val = 'ç™ºå£²'          \n",
    "    elif 'Mã‚¹ãƒ†' in row['text']:\n",
    "        val = 'Mã‚¹ãƒ†'    \n",
    "    elif 'ãŠèª•ç”Ÿæ—¥' in row['text']:\n",
    "        val = 'èª•ç”Ÿæ—¥' \n",
    "    elif 'Amazonã‚®ãƒ•ãƒˆåˆ¸' in row['text'] or 'Amazonã‚®ãƒ•ãƒˆ' in row['text']:\n",
    "        val = 'Amazonã‚®ãƒ•ãƒˆåˆ¸'\n",
    "    elif 'ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯' in row['text']:\n",
    "        val = 'ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯'\n",
    "    elif 'Ï‰' in row['text'] or 'ê’³' in row['text']:\n",
    "        val = 'Ï‰'        \n",
    "    elif row['text']=='å¹´1æœˆ':\n",
    "        val = '1æœˆ'  \n",
    "    elif row['text']=='å¹´2æœˆ':\n",
    "        val = '2æœˆ' \n",
    "    elif row['text']=='å¹´3æœˆ':\n",
    "        val = '3æœˆ' \n",
    "    elif row['text']=='å¹´4æœˆ':\n",
    "        val = '4æœˆ' \n",
    "    elif row['text']=='å¹´12æœˆ':\n",
    "        val = '12æœˆ'                        \n",
    "    elif row['text']=='æ—¥åˆå¾Œ':\n",
    "        val = 'åˆå¾Œ'  \n",
    "    else:\n",
    "        val = row['text']\n",
    "    return val\n",
    "\n",
    "result['text'] = result.apply(merge_keywords, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We won't be using Emojis as part of our analysis, so we delete them here.\n",
    "\n",
    "To_remove = ['ğŸ‘‰','ğŸ’«','ğŸ‘ğŸ»','ğŸ‘','ğŸ˜­','ğŸ‰','ğŸ¤£',\n",
    "             'ğŸ˜…','ğŸ’¦','ğŸ»','ğŸ˜','â¤ï¸','ğŸ¦€','ğŸ˜',\n",
    "             'ğŸ†','ğŸŒŸ','ğŸ˜†','ğŸ‘','ğŸµ','ğŸ·','ğŸ¶',\n",
    "             'ğŸ¥','ğŸ¦ ','ğŸ’•','ğŸ‘‹','ğŸ“£','ğŸŒ¸','ğŸ¯',\n",
    "             'ğŸ˜','ğŸ‘','âš ï¸','ğŸ˜ƒ','â—ï¸','ğŸ’™','ğŸ’¿',\n",
    "             'ğŸ™‡â€','ğŸ™‡â€â™€ï¸','ğŸ¥Ÿ','ğŸ¥º','ğŸŒº','ğŸ’•','ğŸ´',\n",
    "             'ğŸ’¡','ğŸ“¢','ğŸ”¥','ğŸ˜Š','ğŸ¤›','ğŸ˜‚','ğŸ¤­','âœŒï¸',\n",
    "             'â£ï¸','â­ï¸','ğŸš','ğŸ’','ãŠ—ï¸','ğŸŠ','ğŸ”','ğŸ‘‘',\n",
    "             'ğŸ€', 'ğŸ‚', 'ğŸ’›', 'ğŸ’œ', 'ğŸ˜¡',\n",
    "             'æ—¥ç›®','å›ç›®','æšç›®','ã•ã‚“','å·åº—','æ­³å…',\n",
    "             'ä½','å›è©±','åæ§˜','åã•ã¾','m','ï¿£Y']\n",
    "\n",
    "for i in To_remove:\n",
    "    result['text'] = result['text'].str.replace(i,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sentiment Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create \"negative_label_count\", \"neutral_label_count\" and \"positive_label_count\" to represent daily count of 3 labels.\n",
    "sentiment_label = pd.crosstab(result.sentiment_label,result.date).T\n",
    "for x in sentiment_label:\n",
    "    sentiment_label = sentiment_label.rename(columns={x: x+'_label_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Negativity in Every Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.get_dummies(result, columns=['sentiment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 22100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we craete binary variable 'sentiment_label_negative' to represent if a keyword displayed negativity on that day. \n",
    "\n",
    "sentiment_label_negative = pd.DataFrame(result.groupby(['date', 'text'])['sentiment_label_negative'].mean())\n",
    "sentiment_label_negative = sentiment_label_negative.unstack(level=0).T\n",
    "sentiment_label_negative.reset_index(level=0, inplace=True)\n",
    "sentiment_label_negative = sentiment_label_negative.drop('level_0',axis=1)\n",
    "sentiment_label_negative = sentiment_label_negative.iloc[:,1:]\n",
    "sentiment_label_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 578)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete columns containing either 95% or more NaN Values\n",
    "perc = 95\n",
    "min_count =  int(((100-perc)/100)*sentiment_label_negative.shape[0] + 1)\n",
    "sentiment_label_negative = sentiment_label_negative.dropna( axis=1, thresh=min_count)\n",
    "sentiment_label_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 578)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_label_negative = sentiment_label_negative.replace(np.nan, 0)\n",
    "sentiment_label_negative[sentiment_label_negative != 0] = 1\n",
    "sentiment_label_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sentiment_label_negative:\n",
    "    sentiment_label_negative = sentiment_label_negative.rename(columns={x: 'negative '+x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Positivity in Every Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we craete binary variable 'sentiment_label_positive' to represent if a keyword displayed positivity on that day. \n",
    "sentiment_label_positive = pd.DataFrame(result.groupby(['date', 'text'])['sentiment_label_positive'].mean())\n",
    "sentiment_label_positive = sentiment_label_positive.unstack(level=0).T\n",
    "sentiment_label_positive.reset_index(level=0, inplace=True)\n",
    "sentiment_label_positive = sentiment_label_positive.drop('level_0',axis=1)\n",
    "sentiment_label_positive = sentiment_label_positive.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 578)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete columns containing either 95% or more NaN Values\n",
    "perc = 95\n",
    "min_count =  int(((100-perc)/100)*sentiment_label_positive.shape[0] + 1)\n",
    "sentiment_label_positive = sentiment_label_positive.dropna( axis=1, thresh=min_count)\n",
    "sentiment_label_positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_label_positive = sentiment_label_positive.replace(np.nan, 0)\n",
    "sentiment_label_positive[sentiment_label_positive != 0] = 1\n",
    "\n",
    "for x in sentiment_label_positive:\n",
    "    sentiment_label_positive = sentiment_label_positive.rename(columns={x: 'positive '+x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Daily Average Overall Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_score = result.groupby('date').mean()['sentiment_score'].reset_index()\n",
    "sentiment_score = sentiment_score.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Daily Keyword Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_count = pd.DataFrame(result.groupby(['date', 'text'])['count'].sum())\n",
    "keyword_count = keyword_count.unstack(level=0).T\n",
    "keyword_count.reset_index(level=0, inplace=True)\n",
    "keyword_count = keyword_count.drop('level_0',axis=1)\n",
    "keyword_count = keyword_count.iloc[:, 1:]\n",
    "for x in keyword_count:\n",
    "    keyword_count = keyword_count.rename(columns={x: 'count '+x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns containing either 95% or more NaN Values\n",
    "perc = 95\n",
    "min_count =  int(((100-perc)/100)*keyword_count.shape[0] + 1)\n",
    "keyword_count = keyword_count.dropna( axis=1, thresh=min_count)\n",
    "\n",
    "keyword_count = keyword_count.replace(np.nan, 0)\n",
    "keyword_count[keyword_count != 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Create Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we cobine all variables created before and remve constant features in the dataframe. \n",
    "dfALL = pd.concat([sentiment_label, sentiment_label_negative, sentiment_label_positive, keyword_count], axis=1)\n",
    "dfALL = dfALL.replace([np.inf, -np.inf], np.nan)\n",
    "dfALL = dfALL.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 1528)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfALL = dfALL.loc[:, (dfALL != 0).any(axis=0)]\n",
    "dfALL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Percentage Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 581)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nu = pd.concat([sentiment_label, keyword_count], axis=1)\n",
    "df_nu = df_nu.replace([np.inf, -np.inf], np.nan)\n",
    "df_nu = df_nu.fillna(0)\n",
    "df_nu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 581)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we check if there is any empty column.\n",
    "df_nu = df_nu.loc[:, (df_nu != 0).any(axis=0)]\n",
    "df_nu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 581)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfALL_pct = df_nu.pct_change()\n",
    "\n",
    "for x in dfALL_pct:\n",
    "    dfALL_pct = dfALL_pct.rename(columns={x: 'pct '+x}) \n",
    "\n",
    "dfALL_pct = dfALL_pct.replace([np.inf, -np.inf], np.nan)\n",
    "dfALL_pct = dfALL_pct.fillna(0)\n",
    "dfALL_pct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lags ####\n",
    "LAG1 = dfALL.shift(periods=1) \n",
    "LAG2 = dfALL.shift(periods=2) \n",
    "LAG3 = dfALL.shift(periods=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lag difference ####\n",
    "LAG_nu_1 = df_nu.shift(periods=1) \n",
    "LAG_nu_2 = df_nu.shift(periods=2) \n",
    "LAG_nu_3 = df_nu.shift(periods=3) \n",
    "\n",
    "diff_LAG1 = LAG_nu_1 - df_nu\n",
    "diff_LAG2 = LAG_nu_2 - df_nu\n",
    "diff_LAG3 = LAG_nu_3 - df_nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in LAG1:\n",
    "    LAG1 = LAG1.rename(columns={x: 'Lag1 '+x}) \n",
    "for x in LAG2:\n",
    "    LAG2 = LAG2.rename(columns={x: 'Lag2 '+x})\n",
    "for x in LAG3:\n",
    "    LAG3 = LAG3.rename(columns={x: 'Lag3 '+x})\n",
    "    \n",
    "for x in diff_LAG1:\n",
    "    diff_LAG1 = diff_LAG1.rename(columns={x: 'DiffLag1 '+x}) \n",
    "for x in diff_LAG2:\n",
    "    diff_LAG2 = diff_LAG2.rename(columns={x: 'DiffLag2 '+x}) \n",
    "for x in diff_LAG3:\n",
    "    diff_LAG3 = diff_LAG3.rename(columns={x: 'DiffLag3 '+x}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Rolling Windows Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### rolling 3 ####\n",
    "ROLL_3 = df_nu.rolling(3, win_type='triang').mean()\n",
    "#### rolling difference #### \n",
    "DiffRoll3 = ROLL_3 - df_nu\n",
    "\n",
    "for x in ROLL_3:\n",
    "    ROLL_3 = ROLL_3.rename(columns={x: 'Roll3 '+x}) \n",
    "for x in DiffRoll3:\n",
    "    DiffRoll3 = DiffRoll3.rename(columns={x: 'DiffRoll3 '+x}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Output All Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfALL = pd.concat([dfALL, dfALL_pct, LAG1, LAG2, LAG3, diff_LAG1, diff_LAG2, diff_LAG3, ROLL_3, DiffRoll3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 9554)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfALL = dfALL.iloc[3:,:]\n",
    "dfALL = dfALL.replace([np.inf, -np.inf], np.nan)\n",
    "dfALL = dfALL.replace(np.nan, 0)\n",
    "\n",
    "dfALL = dfALL.loc[:, (dfALL != 0).any(axis=0)]\n",
    "dfALL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 9554)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we check if there is any empty column.\n",
    "dfALL = dfALL.loc[:, (dfALL != 0).any(axis=0)]\n",
    "dfALL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 9554)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete columns containing either 95% or more NaN Values\n",
    "perc = 95\n",
    "min_count =  int(((100-perc)/100)*dfALL.shape[0] + 1)\n",
    "dfALL = dfALL.dropna( axis=1, thresh=min_count)\n",
    "dfALL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Label Variables \n",
    "Because R studio cannot display Japanese characters, here we transfer variable names to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\processed data')\n",
    "\n",
    "keyword_list = dfALL.columns.to_list()\n",
    "keyword_list = pd.DataFrame(keyword_list)\n",
    "keyword_list.to_excel('keyword_list_correct.xlsx')\n",
    "\n",
    "df_rename = dfALL\n",
    "for x in range(0,8319):\n",
    "    df_rename.rename(columns={ df_rename.columns[x]: 'keywords_df'+str(x) }, inplace = True) \n",
    "df_rename.to_csv('KEYWORDS_with_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Check Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes long time, so we won't run it in the demonstration. \n",
    "\n",
    "#os.chdir('C:\\\\Users\\\\alexx\\\\Documents\\\\thesis\\\\processed data')\n",
    "\n",
    "#target = pd.read_csv('SALES.csv')\n",
    "\n",
    "#col_choice = dfALL.columns\n",
    "\n",
    "#for x in col_choice:\n",
    "    #plt.scatter(dfALL[x], target['spike'])\n",
    "    #plt.title(x)\n",
    "    #plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
